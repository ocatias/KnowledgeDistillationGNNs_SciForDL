model:  ["GIN"]
batch_size: [128]
emb_dim: [64]
drop_out: [0]
num_layers: [5]
final_mp_layers: [0]
num_mlp_layers: [2]
pooling: ["mean"]
lr: [0.001]
# min_lr: [1e-6]
selection: ["last"]

epochs: [100]
lr_scheduler: ["Cosine"]
lr_schedule_patience: [10]

# Knowledge Distillation
graph_emb: [0]
import_mlp_layer: [0]
node_emb: [0]
node_emb_fac: [10]
m_p_edge_add: [0.05]
m_p_edge_drop: [0.05]
m_p_feat: [0.2]
pred_loss: [1]
teacher: ["GSN9"]
freeze_mlp_layer: [0]